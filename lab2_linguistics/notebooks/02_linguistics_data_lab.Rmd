---
title: "Lab 2 - Linguistics Data"
author: "Tiffany Tang"
date: "`r Sys.Date()`"
output: vthemes::vmodern
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)

source(here::here("R", "load-solution.R"))
source(here::here("R", "clean-solution.R"))
source(here::here("R", "plot.R"))
source(here::here("R", "utils.R"))

DATA_PATH <- here::here("data")
```

# Load Data {.tabset .tabset-vmodern}

Let's first load in the three datasets:

-   `qa_key`: question and answer key
-   `ling_orig`: (uncleaned) linguistics survey data
-   `zip_df`: mapping between ZIP codes and latitude/longitude coordinates

**Your task:**

-   Open `R/load.R` and fill in the code for `load_ling_data()`.

```{r load-data}
# load in data
qa_key <- load_q_and_a_key(DATA_PATH)
ling_orig <- load_ling_data(DATA_PATH)
zip_df <- load_zip_data(DATA_PATH)
```

## Quick Look {.tabset .tabset-pills .tabset-square}

Next, taking a quick look at the data...

### Q and A Data

```{r}
skimr::skim(qa_key)
```

### Linguistics Data

```{r}
skimr::skim(ling_orig)
```

### ZIP Data

```{r}
skimr::skim(zip_df)
```

# Clean Data {.tabset .tabset-vmodern}

In order to facilitate visualizations and future analyses, we will next merge the linguistics survey data with the ZIP locations data and clean the data.

**Your task:**

-   Open `R/clean.R` and fill in the code for `merge_ling_zip_data()`. This function should merge the linguistics survey data with the ZIP locations data. The goal is to relate each individual to their latitude/longitude location coordinates.

```{r}
# merge linguistics survey data with ZIP locations
ling_df <- merge_ling_zip_data(ling_orig, zip_df)
skimr::skim(ling_df)
```

```{r}
ling_df |>
  dplyr::filter(is.na(ZIP_lat) | is.na(ZIP_long)) |>
  dplyr::distinct(ZIP, CITY, STATE)
```

**Your task:**

-   Open `R/clean.R` and fill in the code for `clean_ling_data()`. Feel free to do any data cleaning you would like here.

```{r}
# do data cleaning (if necessary)
ling_df_cleaned <- clean_ling_data(ling_df)
skimr::skim(ling_df_cleaned)
```

# Data Explorations {.tabset .tabset-vmodern}

Let's do some exploratory data analysis!

To get you started, here is a map of the survey respondents and their responses to question 50.

```{r}
plt <- plot_ling_map(ling_df, qa_key, qid = 50)
plt
```

Let's also look at several different questions side-by-side:

```{r}
qids <- c(80, 73, 110, 65)
plt_ls <- list()
for (qid in qids) {
  plt_ls[[as.character(qid)]] <- plot_ling_map(
    ling_df, qa_key, qid = qid
  )
}
plt <- patchwork::wrap_plots(plt_ls)
plt
```

There are lots of categories with very few responses. What if we collapse these rare responses into an "other" category to simplify the visualization?

```{r}
qids <- c(80, 73, 110, 65)
collapsed_out <- collapse_survey_responses(ling_df, qa_key, min_prop = 0.05)
collapsed_ling_df <- collapsed_out$ling_data
collapsed_qa_key <- collapsed_out$qa_key
plt_ls <- list()
for (qid in qids) {
  plt_ls[[as.character(qid)]] <- plot_ling_map(
    collapsed_ling_df, collapsed_qa_key, qid = qid
  ) +
    vthemes::scale_color_vmodern(discrete = TRUE)
}
plt <- patchwork::wrap_plots(plt_ls)
plt
```

Visualizing each survey data point introduces some visual bias towards regions with more responses. Here is a visualization showing the most popular response per county.

```{r}
qids <- c(80, 73, 110, 65)
plt_ls <- list()
for (qid in qids) {
  plt_ls[[as.character(qid)]] <- plot_ling_map_by_county(
    collapsed_ling_df, collapsed_qa_key, qid = qid
  ) +
    vthemes::scale_fill_vmodern(discrete = TRUE)
}
plt <- patchwork::wrap_plots(plt_ls)
plt
```

**Your task:**

-   Spend some time exploring other questions and visualizations of the data. The goal is to get a feel for what the data is like and what kind of dimension reduction/clustering analyses you might want to do in the future.

```{r}
# TO FILL IN
```

# Dimension Reduction {.tabset .tabset-vmodern}

Time to begin dimension reduction.

## Principal Components Analysis {.tabset .tabset-pills .tabset-square}

Let's start with arguably the most popular dimension reduction technique - principal components analysis (PCA) using `prcomp()`.

If we apply PCA without too much thought on our dataset:

```{r}
# do PCA
X <- get_X_matrix(ling_df)
pca_out <- prcomp(X)
pca_plt1 <- plot_pca(
  pca_out, npcs = 2, color = get_answers(ling_df$Q059, qid = 59, qa_key)
)
pca_plt1
```

The first PC corresponds to Q59. Why? Because this question has the most number of choices, so it's encoding ranges from 0-21.

Let's try normalizing the X so that all the questions are on the same scale:

```{r}
pca_out <- prcomp(X, scale = TRUE)
pca_plt2 <- plot_pca(
  pca_out, npcs = 2, color = get_answers(ling_df$Q059, qid = 59, qa_key)
)
pca_plt2
```

But this still doesn't show much of a pattern with the geography of the US, which we would expect from our EDA.

```{r}
plt <- plot_dr_map(pca_out$x, ling_df)
plt
```

The issue is encoding the categorical variables directly to a number. We instead should be one-hot encoding these categorical responses.

```{r}
X <- one_hot_ling_data(ling_df)
pca_out <- prcomp(X)
pca_plt3 <- plot_pca(
  pca_out, npcs = 2, color = get_answers(ling_df$Q059, qid = 59, qa_key)
)
pca_plt3

```

But now, the PCA plot essentially is showing us the variation between people who responded and those who did not respond to the questions.

```{r}
num_nas <- rowSums(get_X_matrix(ling_df) == 0)
hist(num_nas)

pca_plt4 <- plot_pca(
  pca_out, npcs = 2, color = num_nas
)
pca_plt4
```

Let's remove people who left too many questions unanswered and do PCA.

```{r}
ling_df_cleaned <- remove_samples(ling_df, min_answers = 50)
X <- one_hot_ling_data(ling_df_cleaned)
pca_out <- prcomp(X)
pca_plt5 <- plot_pca(
  pca_out, npcs = 2
)
pca_plt5
```

```{r}
plt <- plot_dr_map(pca_out$x, ling_df_cleaned)
plt
```

The first two PCs now show some association with geographical regions in the US.

We can also check out the PC loadings to find the features that are driving the top PCs

```{r}
as.data.frame(abs(pca_out$rotation))
```

## tSNE {.tabset .tabset-pills .tabset-square}

While PCA is a linear dimension reduction method, tSNE is a non-linear dimension reduction method. To perform tSNE, we can use the `Rtsne::Rtsen()` function.

```{r}
# TO FILL IN
```

## UMAP {.tabset .tabset-pills .tabset-square}

Similar to tSNE, UMAP is a non-linear dimension reduction method. To perform UMAP, we can use the `umap::umap()` function.

```{r}
# TO FILL IN
```

## Non-negative matrix factorization (NMF) {.tabset .tabset-pills .tabset-square}

NMF is a linear dimension reduction method that can be applied to non-negative data. Our data is non-negative, so it could be interesting to try this method out to see if exploiting this non-negative structure helps with the dimension reduction. To perform NMF, we can use the `NMF::nmf()` function.

```{r}
# TO FILL IN
```
